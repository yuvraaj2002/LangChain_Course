{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cxS-8GpDiS"
   },
   "source": [
    "## Installing the dependencies\n",
    "The very first step is to install the dependecies, so for that simply open your terminal, create a new virtual environment and run below mentioned commands.\n",
    "\n",
    "- pip install langchain[llms]\n",
    "- pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Setting up openai api key\n",
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7xE34z17vyoo"
   },
   "outputs": [],
   "source": [
    "# Initializing a large language model\n",
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts and Prompt templates ‚úèÔ∏è\n",
    "\n",
    "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.To better understand this let us assume that we want information abuout current temperature in Chandigarh city. So for this our prompt could something be like \"Give me current temperature in Chandigarh\". But what if we want to again find the temperature but for some other city like Delhi. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated city.\n",
    "\n",
    "But, if you are a computer science student you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated city, so to make the process of creating the prompts efficient we use prompts templates.\n",
    "\n",
    "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job role :  ML engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : What is the work of a ML engineer?\n",
      "LLM Output: \n",
      "\n",
      "A ML engineer is responsible for designing, developing, and maintaining machine learning systems and algorithms. This includes training, testing, and optimizing models, writing clean and efficient code, creating experiments, and analyzing results. They are also responsible for researching the latest ML technologies and keeping up with industry trends.\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"What is the work of a {input_parameter}?\")   \n",
    "user_input = input(\"Enter job role : \")\n",
    "prompt = template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)\n",
    "\n",
    "print(\"LLM Output:\",Model.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains üîó\n",
    "\n",
    "In LangChain, a chain is simply a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
    "\n",
    "- Simple chains are typically used as building blocks for more complex chains. For example, a simple chain might take user input, format it into a prompt, and then pass it to an LLM.\n",
    "  \n",
    "- Utility chains are comprised of many LLMs to help solve a specific task. For example, a utility chain might be used to summarize a document, answer questions about a document, or create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 105 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nData scientists work to extract valuable insights from data to help organizations make better, informed decisions. They use analytical approaches such as machine learning, data mining, statistical analysis, and predictive analytics to analyze data from various sources, uncover patterns, and present results in a visual format. As such, they must have strong analytical, technical, communication, and problem-solving skills. Data scientists also have to ensure the accuracy of the data and that it meets the company's specific needs.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)\n",
    "prompt = PromptTemplate(input_variables = ['question'],template=\"{question}\") \n",
    "\n",
    "# Here the simple chain is linking prompt and llm in sequence \n",
    "chain = LLMChain(llm = Model, prompt = prompt)\n",
    "count_tokens(chain,\"What is the work of data scientist\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_data",
   "language": "python",
   "name": "text_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
