{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cxS-8GpDiS"
   },
   "source": [
    "## Installing the dependencies\n",
    "The very first step is to install the dependecies, so for that simply open your terminal, create a new virtual environment and run below mentioned commands.\n",
    "\n",
    "- pip install langchain[llms]\n",
    "- pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Setting up openai api key\n",
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7xE34z17vyoo"
   },
   "outputs": [],
   "source": [
    "# Initializing a large language model\n",
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts and Prompt templates ‚úèÔ∏è\n",
    "\n",
    "A prompt is a textual instruction that we give to a model to provide some specific output. For example, the prompt \"Give me the current temperature in Chandigarh\" would instruct the model to generate a response with the current temperature in Chandigarh.\n",
    "\n",
    "Prompt templates are a way to make the process of creating prompts more efficient. They are essentially pre-made templates that can be customized with different input values. For example, we could create a prompt template that would take the name of a city as input and generate a prompt that asks the model for the current temperature in that city. This would be more efficient than rewriting the prompt from scratch every time we wanted to get the temperature for a different city. It would also make our code more reproducible, since we would only have to update the prompt template once, rather than every time we wanted to get the temperature for a different city.\n",
    "\n",
    "The context that we provide in a prompt template can vary depending on the use case. For example, if we are only interested in the current temperature, then we might not need to provide any context. However, if we are also interested in the humidity, then we might want to provide the context \"current temperature and humidity\" in the prompt template.\r\n",
    "\r\n",
    "The decision of whether or not to provide context in a prompt template is a trade-off between efficiency and accuracy. Providing context can make the prompt more accurate, but it can also make the prompt more complex and less efficient. The best approach will depend on the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] output_parser=None partial_variables={} template='What is the work of a {input}?' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# 1 way of creating a prompt template\n",
    "template1 = PromptTemplate(template = \"What is the work of a {input}\",input_variables = [\"input\"])\n",
    "\n",
    "# 2 way of creating a prompt template ( without manually specifying the input variables )\n",
    "template2 = PromptTemplate.from_template(\"What is the work of a {input}?\")   \n",
    "\n",
    "print(template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job role :  ML engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : What is the work of a ML engineer?\n",
      "LLM Output: \n",
      "\n",
      "A ML Engineer is responsible for building, maintaining, and optimizing Machine Learning models. They must be able to apply their knowledge of mathematics, programming, and Machine Learning algorithms to develop models that can be used to analyze data and make predictions. As a ML engineer, they must also be able to work with large datasets and communicate complex data analysis results in a way that is understandable to other members of the team. Additionally, they are expected to be able to research, evaluate, and compare different Machine Learning algorithms and techniques in order to select the best solution for a given problem.\n"
     ]
    }
   ],
   "source": [
    "# Let see a scenario where we want the end user will feed an input and we will use it in prompt template\n",
    "user_input = input(\"Enter job role : \")\n",
    "prompt_template = PromptTemplate.from_template(\"What is the work of a {input}?\")   \n",
    "\n",
    "# Creating prompt using the user input\n",
    "prompt = prompt_template.format(input=user_input)\n",
    "print(\"Prompt :\",prompt)\n",
    "\n",
    "# Feeding the prompt to LLM\n",
    "print(\"LLM Output:\",Model.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains üîó\n",
    "\n",
    "In LangChain, a chain is simply a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
    "\n",
    "- Simple chains are typically used as building blocks for more complex chains. For example, a simple chain might take user input, format it into a prompt, and then pass it to an LLM.\n",
    "  \n",
    "- Utility chains are comprised of many LLMs to help solve a specific task. For example, a utility chain might be used to summarize a document, answer questions about a document, or create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job role :  ML engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : What is the work of a ML engineer?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA ML engineer is responsible for designing, developing, and managing machine learning models and systems. This includes tasks such as processing and exploring data sets, creating algorithms, testing models, deploying systems, and maintaining them. ML engineers also work to improve existing models and systems, as well as develop innovative ideas for new applications.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)\n",
    "prompt_template = PromptTemplate.from_template(\"What is the work of a {input_parameter}?\")   \n",
    "user_input = input(\"Enter job role : \")\n",
    "\n",
    "# Printing prompt just for visualizing what will go inside the LLM\n",
    "prompt = prompt_template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)\n",
    "\n",
    "# Here the simple chain is linking prompt_template and llm in sequence \n",
    "chain = LLMChain(llm = Model, prompt = template)\n",
    "chain.run(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_data",
   "language": "python",
   "name": "text_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
