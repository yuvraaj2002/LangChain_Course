{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cxS-8GpDiS"
   },
   "source": [
    "## Installing the dependencies\n",
    "The very first step is to install the dependecies, so for that simply open your terminal, create a new virtual environment and run below mentioned commands.\n",
    "\n",
    "- pip install langchain[llms]\n",
    "- pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Setting up openai api key\n",
    "OPENAI_API_KEY = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7xE34z17vyoo"
   },
   "outputs": [],
   "source": [
    "# Initializing a large language model\n",
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts and Prompt templates ‚úèÔ∏è\n",
    "\n",
    "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.To better understand this let us assume that we want information abuout current temperature in Chandigarh city. So for this our prompt could something be like \"Give me current temperature in Chandigarh\". But what if we want to again find the temperature but for some other city like Delhi. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated city.\n",
    "\n",
    "But, if you are a computer science student you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated city, so to make the process of creating the prompts efficient we use prompts templates.\n",
    "\n",
    "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job role :  ML engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : What is the work of a ML engineer?\n",
      "LLM Output: \n",
      "\n",
      "A ML engineer is responsible for designing, developing, and maintaining machine learning systems and algorithms. This includes training, testing, and optimizing models, writing clean and efficient code, creating experiments, and analyzing results. They are also responsible for researching the latest ML technologies and keeping up with industry trends.\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"What is the work of a {input_parameter}?\")   \n",
    "user_input = input(\"Enter job role : \")\n",
    "prompt = template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)\n",
    "\n",
    "print(\"LLM Output:\",Model.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains üîó\n",
    "\n",
    "In LangChain, a chain is simply a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
    "\n",
    "- Simple chains are typically used as building blocks for more complex chains. For example, a simple chain might take user input, format it into a prompt, and then pass it to an LLM.\n",
    "  \n",
    "- Utility chains are comprised of many LLMs to help solve a specific task. For example, a utility chain might be used to summarize a document, answer questions about a document, or create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter job role :  ML engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : What is the work of a ML engineer?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA ML engineer is responsible for designing, developing, and managing machine learning models and systems. This includes tasks such as processing and exploring data sets, creating algorithms, testing models, deploying systems, and maintaining them. ML engineers also work to improve existing models and systems, as well as develop innovative ideas for new applications.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = OpenAI(temperature = 0.9,openai_api_key=OPENAI_API_KEY)\n",
    "prompt_template = PromptTemplate.from_template(\"What is the work of a {input_parameter}?\")   \n",
    "user_input = input(\"Enter job role : \")\n",
    "\n",
    "# Printing prompt just for visualizing what will go inside the LLM\n",
    "prompt = prompt_template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)\n",
    "\n",
    "# Here the simple chain is linking prompt_template and llm in sequence \n",
    "chain = LLMChain(llm = Model, prompt = template)\n",
    "chain.run(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_data",
   "language": "python",
   "name": "text_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
